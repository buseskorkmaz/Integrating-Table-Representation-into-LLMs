METEOR, ROUGE, BLEU, and BERTScore Evaluation Results:
Model: log-test_cl-flan-t5-small-False.txt
METEOR score: 0.059830359006192435
ROUGE scores: {'rouge1': 0.09059458619112513, 'rouge2': 0.03214665313941829, 'rougeL': 0.07132355832628079, 'rougeLsum': 0.07121683551175273}
BLEU score: 2.6058908670542285
BERTScore: 0.7909852862358093

Model: log-test_other-flan-t5-small-False.txt
METEOR score: 0.05961263373197016
ROUGE scores: {'rouge1': 0.08457644306116405, 'rouge2': 0.03010933515344915, 'rougeL': 0.0655722190709242, 'rougeLsum': 0.06590289975371036}
BLEU score: 42.649937722961525
BERTScore: 0.7940241098403931

Model: log-test_cl-flan-t5-base-False.txt
METEOR score: 0.043887170938633466
ROUGE scores: {'rouge1': 0.06396279837397933, 'rouge2': 0.017662864709356865, 'rougeL': 0.050302552875786784, 'rougeLsum': 0.05022216869903963}
BLEU score: 0.42549266588583234
BERTScore: 0.7441673874855042

Model: log-test_other-flan-t5-base-False.txt
METEOR score: 0.04657008797924008
ROUGE scores: {'rouge1': 0.06735424958216904, 'rouge2': 0.016832934299263767, 'rougeL': 0.05152392443156781, 'rougeLsum': 0.05160013382344561}
BLEU score: 5.461616982826483
BERTScore: 0.7352533340454102

Model: log-test_cl-flan-t5-large-False.txt
METEOR score: 0.09941709218643809
ROUGE scores: {'rouge1': 0.1208932252468812, 'rouge2': 0.0340634914930278, 'rougeL': 0.07849583123747597, 'rougeLsum': 0.07828311079073613}
BLEU score: 0.3579376684212033
BERTScore: 0.7865741848945618

Model: log-test_other-flan-t5-large-False.txt
METEOR score: 0.1088349490764644
ROUGE scores: {'rouge1': 0.12394933560337093, 'rouge2': 0.037991757677765, 'rougeL': 0.08213658156262496, 'rougeLsum': 0.08230403201573817}
BLEU score: 5.461616982826483
BERTScore: 0.7894580960273743

Model: log-test_cl-flan-t5-xl-False.txt
METEOR score: 0.07963831302051368
ROUGE scores: {'rouge1': 0.09939662647620567, 'rouge2': 0.024075038478926725, 'rougeL': 0.06808745674090369, 'rougeLsum': 0.06821122264282592}
BLEU score: 5.17309239427654
BERTScore: 0.7821385860443115

Model: log-test_other-flan-t5-xl-False.txt
METEOR score: 0.08055812711779066
ROUGE scores: {'rouge1': 0.09323984738910779, 'rouge2': 0.026100668409645658, 'rougeL': 0.06694266164808807, 'rougeLsum': 0.06716871742343293}
BLEU score: 5.776972987449517
BERTScore: 0.7824397087097168

Model: log-test_cl-Llama-2-7b-chat-hf-False.txt
METEOR score: 0.0769565793729982
ROUGE scores: {'rouge1': 0.06591324423013403, 'rouge2': 0.016990172022617815, 'rougeL': 0.040373237065133766, 'rougeLsum': 0.04045756901029873}
BLEU score: 10.11775206605948
BERTScore: 0.7027972340583801

Model: log-test_other-Llama-2-7b-chat-hf-False.txt
METEOR score: 0.07651558420473005
ROUGE scores: {'rouge1': 0.06534531061251318, 'rouge2': 0.01859456477239565, 'rougeL': 0.04212594122776214, 'rougeLsum': 0.04215982847091458}
BLEU score: 8.70251396295304
BERTScore: 0.7038462162017822

Model: log-test_cl-TableLlama-False.txt
METEOR score: 0.12903925323584994
ROUGE scores: {'rouge1': 0.13853460406324464, 'rouge2': 0.03699746383831701, 'rougeL': 0.09007649654592725, 'rougeLsum': 0.08992433042335596}
BLEU score: 8.423776487963764
BERTScore: 0.772264301776886

Model: log-test_other-TableLlama-False.txt
METEOR score: 0.12554611316806186
ROUGE scores: {'rouge1': 0.1292994217237325, 'rouge2': 0.037646663567064156, 'rougeL': 0.08640077964925322, 'rougeLsum': 0.08653058531565384}
BLEU score: 8.79546152291343
BERTScore: 0.7690573930740356