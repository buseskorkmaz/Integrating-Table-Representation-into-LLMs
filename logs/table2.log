METEOR, ROUGE, BLEU, and BERTScore Evaluation Results:
Model: log-test_cl-flan-t5-small-True.txt
METEOR score: 0.03843242155726298
ROUGE scores: {'rouge1': 0.04684104639224732, 'rouge2': 0.019111211018155803, 'rougeL': 0.03647268525204919, 'rougeLsum': 0.03633994268118532}
BLEU score: 0.3579376684212033
BERTScore: 0.8155796527862549

Model: log-test_other-flan-t5-small-True.txt
METEOR score: 0.03163630340704879
ROUGE scores: {'rouge1': 0.035845386755373784, 'rouge2': 0.014582925654219842, 'rougeL': 0.02867471050056375, 'rougeLsum': 0.02866324023985157}
BLEU score: 0.0
BERTScore: 0.8207847476005554

Model: log-test_cl-flan-t5-base-True.txt
METEOR score: 0.03372149603584964
ROUGE scores: {'rouge1': 0.06560389838940824, 'rouge2': 0.015097905630229138, 'rougeL': 0.055454824753596935, 'rougeLsum': 0.0553941446172119}
BLEU score: 22.894156860669913
BERTScore: 0.8226988315582275

Model: log-test_other-flan-t5-base-True.txt
METEOR score: 0.034480482367382284
ROUGE scores: {'rouge1': 0.06647019208504845, 'rouge2': 0.014194373836707203, 'rougeL': 0.056391522601423924, 'rougeLsum': 0.056155701226203586}
BLEU score: 32.25388307197443
BERTScore: 0.8196374177932739

Model: log-test_cl-flan-t5-large-True.txt
METEOR score: 0.08020741315429669
ROUGE scores: {'rouge1': 0.13800715102154837, 'rouge2': 0.030314623016672038, 'rougeL': 0.11144693073184375, 'rougeLsum': 0.11103654037659949}
BLEU score: 0.533790103142681
BERTScore: 0.7939022183418274

Model: log-test_other-flan-t5-large-True.txt
METEOR score: 0.06898509236499444
ROUGE scores: {'rouge1': 0.11768547531842215, 'rouge2': 0.026075650759715446, 'rougeL': 0.09463409846091536, 'rougeLsum': 0.09475107707245303}
BLEU score: 9.882804650471988
BERTScore: 0.7713351249694824

Model: log-test_cl-flan-t5-xl-True.txt
METEOR score: 0.14060225476018387
ROUGE scores: {'rouge1': 0.2331493058823892, 'rouge2': 0.052244071706110284, 'rougeL': 0.16873466206196672, 'rougeLsum': 0.16842970447594707}
BLEU score: 49.18523839244554
BERTScore: 0.8474248647689819

Model: log-test_other-flan-t5-xl-True.txt
METEOR score: 0.13490307817614425
ROUGE scores: {'rouge1': 0.2318531490250863, 'rouge2': 0.06050164952350433, 'rougeL': 0.17116635924607443, 'rougeLsum': 0.1713245071926524}
BLEU score: 25.56629587057914
BERTScore: 0.8480291962623596

Model: log-test_cl-flan-t5-xl-msr_sqa-True.txt
METEOR score: 0.08114825511848199
ROUGE scores: {'rouge1': 0.10367388043118944, 'rouge2': 0.030628425851051518, 'rougeL': 0.07506931300799638, 'rougeLsum': 0.07542237185418968}
BLEU score: 5.106363606922906
BERTScore: 0.7868887782096863

Model: log-test_other-flan-t5-xl-msr_sqa-True.txt
METEOR score: 0.07741880570194533
ROUGE scores: {'rouge1': 0.09388970007822738, 'rouge2': 0.033324529934197725, 'rougeL': 0.0725556439437807, 'rougeLsum': 0.07284255689549318}
BLEU score: 22.30857686616157
BERTScore: 0.7884771823883057

Model: log-test_cl-flan-t5-xl-wikitable-True.txt
METEOR score: 0.07586362940905166
ROUGE scores: {'rouge1': 0.11683258675528468, 'rouge2': 0.03204676774469004, 'rougeL': 0.08821576694605587, 'rougeLsum': 0.08822388847873433}
BLEU score: 45.09821189657739
BERTScore: 0.8105700016021729

Model: log-test_other-flan-t5-xl-wikitable-True.txt
METEOR score: 0.07134145228123512
ROUGE scores: {'rouge1': 0.1042642872257438, 'rouge2': 0.031227783136984467, 'rougeL': 0.08172263049641623, 'rougeLsum': 0.08191459948825348}
BLEU score: 22.242469397936766
BERTScore: 0.8096032738685608

Model: log-test_cl-wikitable-mrs_sqa-table-True.txt
METEOR score: 0.1400001902978397
ROUGE scores: {'rouge1': 0.23143426361913655, 'rouge2': 0.05353250258374491, 'rougeL': 0.16928471293959363, 'rougeLsum': 0.1693133617243778}
BLEU score: 54.648475850383996
BERTScore: 0.8461225628852844

Model: log-test_other-wikitable-mrs_sqa-table-True.txt
METEOR score: 0.134452319198399
ROUGE scores: {'rouge1': 0.2308800965112791, 'rouge2': 0.0575115341964056, 'rougeL': 0.17036487857240842, 'rougeLsum': 0.17070919225854922}
BLEU score: 34.842683945984476
BERTScore: 0.8469212651252747

Model: log-test_cl-wikitable-scigen-mrs_sqa-table-True.txt
METEOR score: 0.14764654334442873
ROUGE scores: {'rouge1': 0.23961863656173554, 'rouge2': 0.05612093521876626, 'rougeL': 0.17240174919483287, 'rougeLsum': 0.1724458413415912}
BLEU score: 59.382646558960076
BERTScore: 0.8483228087425232

Model: log-test_other-wikitable-scigen-mrs_sqa-table-True.txt
METEOR score: 0.13908740634600114
ROUGE scores: {'rouge1': 0.23317256634410288, 'rouge2': 0.05551500491355771, 'rougeL': 0.1706669188866888, 'rougeLsum': 0.17068663413863927}
BLEU score: 60.44407583380333
BERTScore: 0.846617579460144